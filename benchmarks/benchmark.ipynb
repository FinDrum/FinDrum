{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark compares the performance of the json.loads() and orjson.loads() functions when deserializing JSON data obtained from the SEC. The results show that orjson is significantly faster than the standard json library, making it a preferable choice for our application requiring efficient processing of large volumes of JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average json.loads:   0.041862 seconds\n",
      "Average orjson.loads: 0.020888 seconds\n",
      "Speedup: 2.00x faster with orjson\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import orjson\n",
    "\n",
    "url = \"https://data.sec.gov/api/xbrl/companyfacts/CIK0001318605.json\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"FinDrum Contact <[email protected]>\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "content_bytes = response.content\n",
    "content_str = content_bytes.decode('utf-8')\n",
    "\n",
    "n_iterations = 100\n",
    "times_json = []\n",
    "times_orjson = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    start = time.time()\n",
    "    _ = json.loads(content_str)\n",
    "    times_json.append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    _ = orjson.loads(content_bytes)\n",
    "    times_orjson.append(time.time() - start)\n",
    "\n",
    "mean_json = sum(times_json) / n_iterations\n",
    "mean_orjson = sum(times_orjson) / n_iterations\n",
    "\n",
    "print(f\"\\nAverage json.loads:   {mean_json:.6f} seconds\")\n",
    "print(f\"Average orjson.loads: {mean_orjson:.6f} seconds\")\n",
    "print(f\"Speedup: {mean_json / mean_orjson:.2f}x faster with orjson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyarrow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a Python benchmark script that compares the performance of reading and writing a large DataFrame to a TSV (tab-separated values) format using different parsing engines in pandas: 'c', 'python', and 'pyarrow'. The script measures the average time taken to write to and read from a TSV file over multiple iterations and calculates the relative speedup between engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark for engine: c ---\n",
      "Average write time over 10 runs: 1.843800 sec\n",
      "Average read  time over 10 runs: 0.253963 sec\n",
      "\n",
      "--- Benchmark for engine: python ---\n",
      "Average write time over 10 runs: 1.852542 sec\n",
      "Average read  time over 10 runs: 2.511540 sec\n",
      "\n",
      "--- Benchmark for engine: pyarrow ---\n",
      "Average write time over 10 runs: 1.861710 sec\n",
      "Average read  time over 10 runs: 0.089838 sec\n",
      "\n",
      "Write speedup: python is 1.00x faster than c\n",
      "\n",
      "Read speedup: python is 0.10x faster than c\n",
      "\n",
      "Write speedup: pyarrow is 0.99x faster than c\n",
      "\n",
      "Read speedup: pyarrow is 2.83x faster than c\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"col1\": np.random.randint(0, 1000000, size=1_000_000),\n",
    "    \"col2\": np.random.rand(1_000_000),\n",
    "    \"col3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=1_000_000)\n",
    "})\n",
    "\n",
    "engines = [\"c\", \"python\", \"pyarrow\"]\n",
    "n_iterations = 10\n",
    "results = {engine: {\"write\": [], \"read\": []} for engine in engines}\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n--- Benchmark for engine: {engine} ---\")\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            df.to_csv(buffer, sep='\\t', index=False)\n",
    "            results[engine][\"write\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Write FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            buffer.seek(0)\n",
    "            start = time.time()\n",
    "            pd.read_csv(buffer, sep='\\t', engine=engine)\n",
    "            results[engine][\"read\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Read FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    if results[engine][\"write\"] and results[engine][\"read\"]:\n",
    "        mean_write = sum(results[engine][\"write\"]) / len(results[engine][\"write\"])\n",
    "        mean_read = sum(results[engine][\"read\"]) / len(results[engine][\"read\"])\n",
    "        results[engine][\"mean_write\"] = mean_write\n",
    "        results[engine][\"mean_read\"] = mean_read\n",
    "        print(f\"Average write time over {len(results[engine]['write'])} runs: {mean_write:.6f} sec\")\n",
    "        print(f\"Average read  time over {len(results[engine]['read'])} runs: {mean_read:.6f} sec\")\n",
    "    else:\n",
    "        print(f\"{engine} failed before completing {n_iterations} iterations.\")\n",
    "\n",
    "if all(\"mean_read\" in results[eng] for eng in engines):\n",
    "    base_engine = \"c\"\n",
    "    for engine in engines:\n",
    "        if engine != base_engine:\n",
    "            write_speedup = results[base_engine][\"mean_write\"] / results[engine][\"mean_write\"]\n",
    "            print(f\"\\nWrite speedup: {engine} is {write_speedup:.2f}x faster than {base_engine}\")\n",
    "            read_speedup = results[base_engine][\"mean_read\"] / results[engine][\"mean_read\"]\n",
    "            print(f\"\\nRead speedup: {engine} is {read_speedup:.2f}x faster than {base_engine}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark compares the performance of two Parquet read/write engines in Python: pyarrow and fastparquet. A large DataFrame with 1 million rows is generated, and over the course of 100 iterations, the time taken by each engine to write the DataFrame to an in-memory buffer (BytesIO) and then read it back is measured. At the end, the average read and write times for each engine are calculated, and a relative speedup is determinedâ€”indicating how many times faster pyarrow is compared to fastparquet. This type of benchmark is useful for selecting the most efficient engine in data-intensive processing workflows where I/O performance is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Benchmark for engine: pyarrow ---\n",
      "Average write time over 100 runs: 0.133123 sec\n",
      "Average read  time over 100 runs: 0.029747 sec\n",
      "\n",
      "--- Benchmark for engine: fastparquet ---\n",
      "Average write time over 100 runs: 0.101856 sec\n",
      "Average read  time over 100 runs: 0.030074 sec\n",
      "\n",
      "Speedup (pyarrow vs fastparquet):\n",
      "Write speedup: 0.77x faster using pyarrow\n",
      "Read  speedup: 1.01x faster using pyarrow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"col1\": np.random.randint(0, 1000000, size=1_000_000),\n",
    "    \"col2\": np.random.rand(1_000_000),\n",
    "    \"col3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=1_000_000)\n",
    "})\n",
    "\n",
    "engines = [\"pyarrow\", \"fastparquet\"]\n",
    "n_iterations = 100\n",
    "results = {engine: {\"write\": [], \"read\": []} for engine in engines}\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n--- Benchmark for engine: {engine} ---\")\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        buffer = io.BytesIO()\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            df.to_parquet(buffer, engine=engine, index=False)\n",
    "            results[engine][\"write\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Write FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            buffer.seek(0)\n",
    "            start = time.time()\n",
    "            df_read = pd.read_parquet(buffer, engine=engine)\n",
    "            results[engine][\"read\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Read FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    if results[engine][\"write\"] and results[engine][\"read\"]:\n",
    "        mean_write = sum(results[engine][\"write\"]) / len(results[engine][\"write\"])\n",
    "        mean_read = sum(results[engine][\"read\"]) / len(results[engine][\"read\"])\n",
    "        results[engine][\"mean_write\"] = mean_write\n",
    "        results[engine][\"mean_read\"] = mean_read\n",
    "        print(f\"Average write time over {len(results[engine]['write'])} runs: {mean_write:.6f} sec\")\n",
    "        print(f\"Average read  time over {len(results[engine]['read'])} runs: {mean_read:.6f} sec\")\n",
    "    else:\n",
    "        print(f\"{engine} failed before completing {n_iterations} iterations.\")\n",
    "\n",
    "if all(\"mean_write\" in results[eng] for eng in engines):\n",
    "    write_speedup = results[\"fastparquet\"][\"mean_write\"] / results[\"pyarrow\"][\"mean_write\"]\n",
    "    read_speedup = results[\"fastparquet\"][\"mean_read\"] / results[\"pyarrow\"][\"mean_read\"]\n",
    "\n",
    "    print(f\"\\nSpeedup (pyarrow vs fastparquet):\")\n",
    "    print(f\"Write speedup: {write_speedup:.2f}x faster using pyarrow\")\n",
    "    print(f\"Read  speedup: {read_speedup:.2f}x faster using pyarrow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark evaluates the impact of row ordering on the size of Parquet files using the fastparquet engine. In each of the 100 iterations, a large DataFrame with 1 million rows is generated with random numerical and categorical data. The same data is then written to Parquet twice: once in its original (unsorted) order, and once sorted by two columns (col1 and col3). The resulting file sizes (measured in memory using BytesIO) are recorded and averaged. The benchmark concludes by calculating the average size reduction achieved by sorting the data before serialization. This test demonstrates how sorting can improve Parquet compression efficiency, which is useful in storage-sensitive or I/O-bound data workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Unsorted Parquet size: 12945.37 KB\n",
      "Average Sorted   Parquet size: 8557.88 KB\n",
      "Average Reduction: 33.89% over 100 iterations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "engine = \"fastparquet\"\n",
    "n_iterations = 100\n",
    "\n",
    "unsorted_sizes = []\n",
    "sorted_sizes = []\n",
    "\n",
    "def get_parquet_size(dataframe: pd.DataFrame) -> int:\n",
    "    buffer = io.BytesIO()\n",
    "    dataframe.to_parquet(buffer, engine=engine, index=False)\n",
    "    return buffer.getbuffer().nbytes\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    df = pd.DataFrame({\n",
    "        \"col1\": np.random.randint(0, 10000, size=1_000_000),\n",
    "        \"col2\": np.random.rand(1_000_000),\n",
    "        \"col3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=1_000_000)\n",
    "    })\n",
    "\n",
    "    df_sorted = df.sort_values(by=[\"col1\", \"col3\"]).reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        unsorted_sizes.append(get_parquet_size(df))\n",
    "        sorted_sizes.append(get_parquet_size(df_sorted))\n",
    "    except Exception as e:\n",
    "        print(f\"Iteration {i} FAILED: {e}\")\n",
    "        break\n",
    "\n",
    "unsorted_sizes_kb = [s / 1024 for s in unsorted_sizes]\n",
    "sorted_sizes_kb = [s / 1024 for s in sorted_sizes]\n",
    "\n",
    "mean_unsorted = sum(unsorted_sizes_kb) / len(unsorted_sizes_kb)\n",
    "mean_sorted = sum(sorted_sizes_kb) / len(sorted_sizes_kb)\n",
    "reduction = (1 - mean_sorted / mean_unsorted) * 100\n",
    "\n",
    "print(f\"\\nAverage Unsorted Parquet size: {mean_unsorted:.2f} KB\")\n",
    "print(f\"Average Sorted   Parquet size: {mean_sorted:.2f} KB\")\n",
    "print(f\"Average Reduction: {reduction:.2f}% over {n_iterations} iterations\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
