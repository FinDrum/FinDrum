{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests orjson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This benchmark compares the performance of the json.loads() and orjson.loads() functions when deserializing JSON data obtained from the SEC. The results show that orjson is significantly faster than the standard json library, making it a preferable choice for our application requiring efficient processing of large volumes of JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import json\n",
    "import orjson\n",
    "\n",
    "url = \"https://data.sec.gov/api/xbrl/companyfacts/CIK0001318605.json\"\n",
    "headers = {\n",
    "    \"User-Agent\": \"FinDrum Contact <[email protected]>\"\n",
    "}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "response.raise_for_status()\n",
    "content_bytes = response.content\n",
    "content_str = content_bytes.decode('utf-8')\n",
    "\n",
    "n_iterations = 100\n",
    "times_json = []\n",
    "times_orjson = []\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    start = time.time()\n",
    "    _ = json.loads(content_str)\n",
    "    times_json.append(time.time() - start)\n",
    "\n",
    "    start = time.time()\n",
    "    _ = orjson.loads(content_bytes)\n",
    "    times_orjson.append(time.time() - start)\n",
    "\n",
    "mean_json = sum(times_json) / n_iterations\n",
    "mean_orjson = sum(times_orjson) / n_iterations\n",
    "\n",
    "print(f\"\\nAverage json.loads:   {mean_json:.6f} seconds\")\n",
    "print(f\"Average orjson.loads: {mean_orjson:.6f} seconds\")\n",
    "print(f\"Speedup: {mean_json / mean_orjson:.2f}x faster with orjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyarrow fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import io\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"col1\": np.random.randint(0, 1000000, size=1_000_000),\n",
    "    \"col2\": np.random.rand(1_000_000),\n",
    "    \"col3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=1_000_000)\n",
    "})\n",
    "\n",
    "engines = [\"pyarrow\", \"fastparquet\"]\n",
    "n_iterations = 100\n",
    "results = {engine: {\"write\": [], \"read\": []} for engine in engines}\n",
    "\n",
    "for engine in engines:\n",
    "    print(f\"\\n--- Benchmark for engine: {engine} ---\")\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        buffer = io.BytesIO()\n",
    "\n",
    "        try:\n",
    "            start = time.time()\n",
    "            df.to_parquet(buffer, engine=engine, index=False)\n",
    "            results[engine][\"write\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Write FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            buffer.seek(0)\n",
    "            start = time.time()\n",
    "            df_read = pd.read_parquet(buffer, engine=engine)\n",
    "            results[engine][\"read\"].append(time.time() - start)\n",
    "        except Exception as e:\n",
    "            print(f\"Read FAILED on iteration {i}: {e}\")\n",
    "            break\n",
    "\n",
    "    if results[engine][\"write\"] and results[engine][\"read\"]:\n",
    "        mean_write = sum(results[engine][\"write\"]) / len(results[engine][\"write\"])\n",
    "        mean_read = sum(results[engine][\"read\"]) / len(results[engine][\"read\"])\n",
    "        results[engine][\"mean_write\"] = mean_write\n",
    "        results[engine][\"mean_read\"] = mean_read\n",
    "        print(f\"Average write time over {len(results[engine]['write'])} runs: {mean_write:.6f} sec\")\n",
    "        print(f\"Average read  time over {len(results[engine]['read'])} runs: {mean_read:.6f} sec\")\n",
    "    else:\n",
    "        print(f\"{engine} failed before completing {n_iterations} iterations.\")\n",
    "\n",
    "if all(\"mean_write\" in results[eng] for eng in engines):\n",
    "    write_speedup = results[\"fastparquet\"][\"mean_write\"] / results[\"pyarrow\"][\"mean_write\"]\n",
    "    read_speedup = results[\"fastparquet\"][\"mean_read\"] / results[\"pyarrow\"][\"mean_read\"]\n",
    "\n",
    "    print(f\"\\nSpeedup (pyarrow vs fastparquet):\")\n",
    "    print(f\"Write speedup: {write_speedup:.2f}x faster using pyarrow\")\n",
    "    print(f\"Read  speedup: {read_speedup:.2f}x faster using pyarrow\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "engine = \"fastparquet\"\n",
    "n_iterations = 100\n",
    "\n",
    "unsorted_sizes = []\n",
    "sorted_sizes = []\n",
    "\n",
    "def get_parquet_size(dataframe: pd.DataFrame) -> int:\n",
    "    buffer = io.BytesIO()\n",
    "    dataframe.to_parquet(buffer, engine=engine, index=False)\n",
    "    return buffer.getbuffer().nbytes\n",
    "\n",
    "for i in range(n_iterations):\n",
    "    df = pd.DataFrame({\n",
    "        \"col1\": np.random.randint(0, 10000, size=1_000_000),\n",
    "        \"col2\": np.random.rand(1_000_000),\n",
    "        \"col3\": np.random.choice([\"A\", \"B\", \"C\", \"D\"], size=1_000_000)\n",
    "    })\n",
    "\n",
    "    df_sorted = df.sort_values(by=[\"col1\", \"col3\"]).reset_index(drop=True)\n",
    "\n",
    "    try:\n",
    "        unsorted_sizes.append(get_parquet_size(df))\n",
    "        sorted_sizes.append(get_parquet_size(df_sorted))\n",
    "    except Exception as e:\n",
    "        print(f\"Iteration {i} FAILED: {e}\")\n",
    "        break\n",
    "\n",
    "unsorted_sizes_kb = [s / 1024 for s in unsorted_sizes]\n",
    "sorted_sizes_kb = [s / 1024 for s in sorted_sizes]\n",
    "\n",
    "mean_unsorted = sum(unsorted_sizes_kb) / len(unsorted_sizes_kb)\n",
    "mean_sorted = sum(sorted_sizes_kb) / len(sorted_sizes_kb)\n",
    "reduction = (1 - mean_sorted / mean_unsorted) * 100\n",
    "\n",
    "print(f\"\\nAverage Unsorted Parquet size: {mean_unsorted:.2f} KB\")\n",
    "print(f\"Average Sorted   Parquet size: {mean_sorted:.2f} KB\")\n",
    "print(f\"Average Reduction: {reduction:.2f}% over {n_iterations} iterations\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
